{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# objectif : scraper planet grimpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importation des librairies\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exemple de page :\n",
    "url = 'https://planetgrimpe.com/falaise/la-speleologue/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "if response.ok:\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Falaise la Spéleologue, guide d’escalade le Thuit 27700. Topos, accessibilité, photos et avis des grimpeurs sur la Spéleologue\n"
     ]
    }
   ],
   "source": [
    "if response.ok:\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    title = soup.find('title')\n",
    "    print(title.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Falaise la Spéleologue\n",
      "France \n",
      "Haute Normandie \n",
      "Eure \n",
      "le Thuit 27700\n",
      "15 - 33m\n",
      "70\n",
      "Printemps\n",
      "6a 8b\n",
      "Facile | 10min\n",
      "Sud\n",
      "15m\n",
      "33m\n",
      "25m\n",
      "Couenne\n",
      "49.253475\n",
      "1.358854\n"
     ]
    }
   ],
   "source": [
    "# les infos qui nous intéressent :\n",
    "\n",
    "if response.ok:\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    \n",
    "    spot = soup.find('div', {'id': 'single_top'}).find('h1')\n",
    "    print(spot.text)\n",
    "    \n",
    "    localisation = soup.find('div', {'id': 'single_top'}).findAll('li')\n",
    "    #for i in pays:\n",
    "    #    print(i.text)\n",
    "    print(localisation[0].text)\n",
    "    print(localisation[1].text)\n",
    "    print(localisation[2].text)\n",
    "    print(localisation[3].text)\n",
    "    \n",
    "    infosPrincipalesSpot = soup.find('div', {'id': 'single_top'}).find('div', {'class': 'single_top_infos'}).findAll('div', {'class': 'single_top_infosbloc'})\n",
    "    #for i in infosPrincipalesSpot:\n",
    "    #    print(i.text)\n",
    "    \n",
    "    hauteur = infosPrincipalesSpot[0].text\n",
    "    print(hauteur[:-7])\n",
    "    lignes = infosPrincipalesSpot[1].text\n",
    "    print(lignes[:-6])\n",
    "    meilleuresSaisons = infosPrincipalesSpot[2].text\n",
    "    print(meilleuresSaisons[:-18])\n",
    "    cotations = infosPrincipalesSpot[3].text\n",
    "    print(cotations[:-9])\n",
    "    approche = infosPrincipalesSpot[4].text\n",
    "    print(approche[:-29])\n",
    "    \n",
    "    infosSupp = soup.find('div', {'id': 'page_ctn'}).find('ul').findAll('li')     \n",
    "    orientation = infosSupp[1].text\n",
    "    print(orientation[22:])\n",
    "    hauteurMin = infosSupp[3].text\n",
    "    print(hauteurMin[16:])\n",
    "    hauteurMax = infosSupp[4].text\n",
    "    print(hauteurMax[16:])\n",
    "    hauteurMoyenne = infosSupp[5].text\n",
    "    print(hauteurMoyenne[16:])\n",
    "    discipline = infosSupp[6].text\n",
    "    print(discipline[16:])\n",
    "    \n",
    "    coordonnees = soup.find('div', {'class': 'map_legende'}).find('span')\n",
    "    coord = coordonnees.text\n",
    "    coordSplit = coord.split('-')\n",
    "    coordSplit0 = coordSplit[0]\n",
    "    coordSplit1 = coordSplit[1]\n",
    "    latitude = coordSplit0[11:20]\n",
    "    longitude = coordSplit1[1:9]\n",
    "    #print(coordSplit0)\n",
    "    #print(coordSplit1)\n",
    "    print(latitude)\n",
    "    print(longitude)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('spotsPlanetGrimp.csv', 'w') as outf:\n",
    "    outf.write('spot; pays; region; departement; ville; hauteur; nb_lignes; meileures_saisons; cotations; approche; orientation; hauteur_min; hauteur_max; hauteur_moyenne; type; latitude; longitude\\n')\n",
    "    response = requests.get(url)\n",
    "    if response.ok:\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        \n",
    "        spot = soup.find('div', {'id': 'single_top'}).find('h1')\n",
    "    \n",
    "        localisation = soup.find('div', {'id': 'single_top'}).findAll('li')\n",
    "    \n",
    "        infosPrincipalesSpot = soup.find('div', {'id': 'single_top'}).find('div', {'class': 'single_top_infos'}).findAll('div', {'class': 'single_top_infosbloc'})\n",
    "        \n",
    "        hauteur = infosPrincipalesSpot[0].text\n",
    "        lignes = infosPrincipalesSpot[1].text\n",
    "        meilleuresSaisons = infosPrincipalesSpot[2].text\n",
    "        cotations = infosPrincipalesSpot[3].text\n",
    "        approche = infosPrincipalesSpot[4].text\n",
    "        \n",
    "        infosSupp = soup.find('div', {'id': 'page_ctn'}).find('ul').findAll('li')     \n",
    "        orientation = infosSupp[1].text\n",
    "        hauteurMin = infosSupp[3].text\n",
    "        hauteurMax = infosSupp[4].text\n",
    "        hauteurMoyenne = infosSupp[5].text\n",
    "        discipline = infosSupp[6].text\n",
    "    \n",
    "        coordonnees = soup.find('div', {'class': 'map_legende'}).find('span')\n",
    "        coord = coordonnees.text\n",
    "        coordSplit = coord.split('-')\n",
    "        coordSplit0 = coordSplit[0]\n",
    "        coordSplit1 = coordSplit[1]\n",
    "        latitude = coordSplit0[11:20]\n",
    "        longitude = coordSplit1[1:9]\n",
    "        \n",
    "        outf.write(spot.text +'; '\n",
    "                   +localisation[0].text+'; '\n",
    "                   +localisation[1].text+'; '\n",
    "                   +localisation[2].text+'; '\n",
    "                   +localisation[3].text+'; '\n",
    "                   +hauteur[:-7]+'; '\n",
    "                   +lignes[:-6]+'; '\n",
    "                   +meilleuresSaisons[:-18]+'; '\n",
    "                   +cotations[:-9]+'; '\n",
    "                   +approche[:-29]+'; '\n",
    "                   +orientation[22:]+'; '\n",
    "                   +hauteurMin[16:]+'; '\n",
    "                   +hauteurMax[16:]+'; '\n",
    "                   +hauteurMoyenne[16:]+'; '\n",
    "                   +discipline[16:]+'; '\n",
    "                   +latitude+'; '\n",
    "                   +longitude+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
